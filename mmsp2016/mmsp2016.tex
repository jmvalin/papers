% Originally from a LyX file
\documentclass[english,conference,10pt]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{units}
\begin{document}

\title{Daala: Building A Next-Generation Video Codec From Unconventional
Technology}


\author{\author{\IEEEauthorblockN{Some authors\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Mozilla Corporation, Mountain View, USA}\\
\IEEEauthorblockA{\IEEEauthorrefmark{2}Xiph.Org Foundation, USA}
}}
\maketitle
\begin{abstract}
Daala is a new royalty-free video codec based on perceptually-driven
coding techniques.
\end{abstract}


\section{Introduction}

Daala is a royalty-free video codec designed to avoid traditional
patent-encumbered techniques used in most current video codecs. One
of the goals of the Daala codec is to explore techniques that are
very different from those typically used in most codecs.


\section{Daala Techniques}

One of the goals of the Daala codec is to explore techniques that
are very different from those typically used in most codecs. Most
of these techniques have been fundamental to Daala since the initial
stages of the project. 


\subsection{Lapping}

Daala uses lapped transforms~\cite{MalvarS89,Tran2003} rather than
a regular DCT followed by a deblocking filter. 

This reduces blocking artifacts but prevents the use of standard pixel-based
intra-prediction techniques~\cite{DaedeDCC}. Instead, we use a simple
frequency-domain intra predictor that only handles horizontal and
vertical directions~\cite{EggePCS}. 

Also, DC coefficients are combined recursively using a Haar transform,
up to the level of 64x64 superblocks. 


\subsection{Multi-Symbol Entropy Coder}

Most recent video codecs encode information using binary arithmetic
coding, meaning that each symbol can only take two values. The Daala
range coder supports up to 16 values per symbol, making it possible
to encode fewer symbols~\cite{derfTools}. This is equivalent to
coding up to four binary values in parallel and reduces serial dependencies. 

To achieve this, Daala uses a multiply-free range coder approximation~\cite{stuiver1998piecewise}. 


\subsection{Overlapped Block Motion Compensation}

Lapping -> Straight BMA is a no-go


\subsection{Perceptual Vector Quantization}

In the majority of video codecs, motion-compensated reference frame
is subtracted from the input frame to compute a residual, which is
then transformed, and coded using scalar quantization. Daala differs
significantly from that description. Not only does it use vector quantization
rather than scalar quantization, but -- most importantly -- the motion-compensated
reference is never subtracted from the input frame. Instead, both
the motion-compensated reference is used to build a transformation
that makes the input easier to code. This technique is called perceptual
vector quantization (PVQ)~\cite{valin2015spie}.

Perceptual vector quantization originates in the pyramid vector quantizer
previously used for music in the Opus audio codec~\cite{ValinAES}. In Opus,
the pyramid vector quantizer is used as a gain-shape quantizer in a way that
ensures that the signal energy is always conserved. Using a gain-shape
quantizer in a video codec is more complicated, since we need to also take
into account a prediction. While it would be possible to quantize the difference
between the prediction and the input, conserving the energy of that
difference would be perceptually meaningless.

Rather than attempting to encode the difference, the prediction produces
a Householder reflection that makes the input easier to encode. Let 
$\mathbf{x}$ be the input and $\mathbf{r}$ be the prediction, we construct a Householder reflection plane
\begin{equation}
\mathbf{v} = \frac{\mathbf{r}}{\|\mathbf{r}\|} + s\mathbf{e}_m\ ,
\end{equation}
where $\mathbf{e}_m$ is a unit vector along dimension $m$ and $s = \pm1$.
The values of $m$ and $s$ can be chosen arbitrarily, but to maximize
numerical stability, we typically choose $m$ to be the position of the
largest absolute value in $\mathbf{r}$ and $s$ to be the sign of that value.

Once the reflection plane is computed, it is applied to the input vector $\mathbf{x}$ to produce the reflected vector $\mathbf{z}$:
\begin{equation}
\mathbf{z} = \mathbf{x} - 2\frac{\mathbf{x}^T\mathbf{v}}
{\mathbf{v}^T\mathbf{v}}\mathbf{v}\ .
\end{equation}
When the input is similar to the prediction itself, the direction of the
reflected vector $\mathbf{z}$ is close $-\mathbf{e}_m$. To take advantage of
that fact, we express it as
\begin{equation}
\mathbf{z} = g\left(-s\cos\theta + \mathbf{u}\sin\theta\right)\ ,
\end{equation}
where $g$ is the magnitude of $\mathbf{z}$ (and thus also the magnitude of
$\mathbf{x}$), $\mathbf{u}$ is a unit vector with no component along the
$\mathbf{e}_m$ direction, and the angle $\theta$ representing the
similarity between the prediction and the input. Since the Householder
reflection is orthonormal, it follows that
\begin{equation}
\theta = \arccos\frac{\mathbf{x}^T\mathbf{r}}
                   {\left\|\mathbf{x}\right\|\left\|\mathbf{r}\right\|}\ .
\end{equation}

The unit vector $\mathbf{u}$ can be coded using a spherical quantizer derived
from the pyramid vector quantizer~\cite{Fischer1986}:
\begin{equation}
\mathbf{u}=\frac{\mathbf{y}}{\left\|\mathbf{y}\right\|}\ ,
\end{equation}
with
\begin{equation}
\mathbf{y} \in \mathbb{Z}^N : \left\|\mathbf{y}\right\|_{L1} = K \land y_m=0\ ,
\end{equation}
where the number of \textit{pulses} $K$ controls the size of the codebook.

The encoder quantizes $g$ and $\theta$ and encodes them in the bitstream along
with the integer vector $\mathbf{y}$ (except for $y_m=0$). The codebook
size $K$ is determined only from $g$ and $theta$ and does not need to be
transmitted. Since the decoder also has access to the prediction, the
reflection vector $\mathbf{v}$ and the values $m$ and $s$ do not need to
be transmitted. In total, there are $N-2$ degrees of freedom to code in $\mathbf{y}$, with two more for $g$ and $\theta$, so the total is still
$N$ degrees of freedom. The main difference however is that two of the
coded values have a perceptual meaning: $g$ is the amount of contrast and
$\theta$ is the amount of change compared to the prediction. By coding $g$
as a parameter, it is easier to preserve the amount of contrast than by
coding only DCT coefficients.

In practice, the vectors $\mathbf{x}$ and $\mathbf{r}$ are transform
coefficients rather than pixel values. This requires an extra forward DCT
in both the encoder and the decoder since the input and the prediction need
to be transformed separately. Only the AC coefficients are coded using PVQ
and for blocks larger than 4x4, the AC coefficients are divided into multiple
\textit{bands} where each band is coded separately. This makes it possible
to control the contrast separately, based on octave and direction.

PVQ makes it possible to take into account masking effects with no
extra signaling. Since the gain is explicitly signaled, it is easy to make
the quantization resolution dependent on the gain. This dependency can be
expressed as
\begin{equation}
E\left\lbrace \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2 \right\rbrace
\propto g^{2\alpha}\ , 0 \leq \alpha \leq 1\ ,
\end{equation}
where $\alpha=0$ behaves like a standard linear scalar quantizer and
$\alpha=1$ produces a constant relative error like in the Opus audio codec.
Daala uses $\alpha = \nicefrac{1}{3}$. To achieve this, we quantize the
companded gain
\begin{equation}
\gamma = g^{1-\alpha}\ .
\end{equation}

The decoder always decodes the quantized companded gain $\hat{\gamma}$
first. From there it can compute the quantization step size for $\theta$ as
\begin{equation}
Q_\theta = \frac{\pi}{2\beta}{\hat{\gamma}}\ ,
\end{equation}
where $\beta = 1/(1-\alpha)$.

\subsection{Chroma from Luma (CfL) Prediction}

Because of its structure, PVQ makes it especially easy to predict
chroma planes from the luma plane. Daala's chroma from luma (CfL)~\cite{egge2015spie}
prediction uses the luma transform coefficients to predict the chroma
transform coefficients.


\subsection{Directional Deringing Filter}

Like other transform codecs, Daala can cause ringing artifacts around edges.
The directional deringing filter attempts to eliminate the ringing without
blurring the image. Unlike the HEVC Sample-Adaptive Offset (SAO)~\cite{HEVC-SAO},
the Daala deringing filter is not based on classifying pixels and applying per-class
offsets. Instead, it is a directional outlier-robust filter that smooths the
neighborhood of pixels while preserving edges.

Let $x\left(n\right)$ denote a 1-dimensional signal and $w_{k}$
denote filter tap weights, a linear finite impulse response (FIR)
filter with unit DC response is defined as
\begin{equation}
y\left(n\right)=\frac{1}{\sum_{k}w_{k}}\sum_{k}w_{k}x\left(n+k\right)\ ,\label{eq:FIR1}
\end{equation}
which can alternatively be written as
\begin{equation}
y\left(n\right)=x\left(n\right)+\frac{1}{\sum_{k}w_{k}}\sum_{k,k\neq0}w_{k}\left[x\left(n+k\right)-x\left(n\right)\right]\ .\label{eq:FIR2}
\end{equation}
The main advantage of expressing a filter in the form of Eq.~(\ref{eq:FIR2})
is that the normalization term $\frac{1}{\sum_{k}w_{k}}$ can be approximated
relatively coarsely without affecting the unit gain for DC. This makes
it easy to use small integers for the weights $w_{k}$.

The disadvantage of linear filters for removing ringing artifacts
is that they tend to also cause blurring. To reduce the amount of
blurring, the conditional replacement filter used in Daala excludes
the signal taps $x\left(n+k\right)$ that would cause blurring and
replaces them with $x\left(n\right)$ instead. This is determined
by whether $x\left(n+k\right)$ differs from $x\left(n\right)$ by
more than a threshold $T$. The FIR filter in Eq.~(\ref{eq:FIR2})
then becomes a conditional replacement filter expressed as
\begin{equation}
y\left(n\right)=x\left(n\right)+\frac{1}{\sum_{k}w_{k}}\sum_{k,k\neq0}w_{k}R\left(x\left(n+k\right)-x\left(n\right),T\right)\ ,\label{eq:CRF}
\end{equation}
where
\begin{equation}
R\left(x,T\right)=\left\{ \begin{array}{ll}
x & ,\ \left|x\right|<T\\
0 & ,\ \mathrm{otherwise}
\end{array}\right.\ .
\end{equation}

To further reduce the risk of blurring the decoded image, the conditional
replacement filter is applied along the main direction of the edges
in each 8x8 block. The direction is determined based on the decoded image
(no side information transmitted) by analyzing each 8x8 block as described
in~\cite{ValinDeringing}. For each 8x8 block, the decoder determines which
of eight different directions best represents the content of the block.
The search can be efficiently implemented in SIMD. To filter a pixel, a 7-tap
conditional replacement filter is applied along the detected direction.
The process is repeated for each pixel in each block being filtered.

To further reduce ringing in very smooth regions of the image, the filter
is applied a second time to combine multiple output values of the
first filter. The second filter is applied either vertically or horizontally
-- in the direction most orthogonal to the one used in the first filter.
For example, for a 45-degree direction, the second filter would be
applied horizontally. The combined effect of the two filters is a separable
deringing filter that covers a total of 35~pixel taps.


\section{Alliance for Open Media AV1 codec}
Some of the

\section{Results}


\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{daala}

\end{document}
